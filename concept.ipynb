{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept for template matching\n",
    "## Goals\n",
    "- Faster & accurate template matching\n",
    "- Smaller size of load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAYSTACK = \"./img/2.jpg\"\n",
    "NEEDLE = \"./img/avatars/char_120_hibisc.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(img):\n",
    "    akaze = cv.AKAZE_create()\n",
    "\n",
    "    keypoints, descriptor = akaze.detectAndCompute(img, None)\n",
    "\n",
    "    return keypoints, descriptor\n",
    "\n",
    "def brute_force_matcher(descriptor_query, descriptor_train, sort=False):\n",
    "    matcher = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
    "    matches = matcher.match(descriptor_query, descriptor_train)\n",
    "\n",
    "    if sort == True:\n",
    "        matches = sorted(matches, key=lambda x:x.distance)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def find_anchor(img):\n",
    "    # find region of interest\n",
    "    h_end = img.shape[0]\n",
    "    h_start = int(h_end*0.65)\n",
    "\n",
    "    region = img[h_start:h_end, :]\n",
    "    feature = blur_and_canny(region)\n",
    "    _, _, w, _ = find_box(feature)\n",
    "\n",
    "    return w\n",
    "\n",
    "# refer to line number 43 and 47\n",
    "# should note that sliding window will process from the right half of the image then the left half\n",
    "# since the process is done beginning from the center of the image, then if it were to be enumerated\n",
    "# the 6th window will be 0th index and the 1st window is the 9th index\n",
    "def sliding_window(img, stride):\n",
    "    # crop into two halves\n",
    "    left_img, right_img = crop_image_half(img)\n",
    "    \n",
    "    offset = 120 - stride\n",
    "\n",
    "    if stride < 80: offset -= 20\n",
    "    elif stride < 90: offset -= 15\n",
    "\n",
    "    shift = stride + offset\n",
    "\n",
    "    for i in range(5):\n",
    "        yield right_img[:, shift*i:shift*(i+1)]\n",
    "\n",
    "    _, wi = left_img.shape\n",
    "    for i in range(5):\n",
    "        # left[:, wi-shift*(i+1):wi-shift*i]\n",
    "        yield left_img[:, wi-shift*(i+1):wi-shift*i]\n",
    "\n",
    "def crop_image_half(img):\n",
    "    _, w = img.shape\n",
    "\n",
    "    left_half = img[:, 0:int(w/2)]\n",
    "    right_half = img[:, int(w/2):w]\n",
    "\n",
    "    return left_half, right_half\n",
    "\n",
    "def resize_image(img, width=1280, height=720):\n",
    "    resized = cv.resize(img, (width, height))\n",
    "    \n",
    "    return resized\n",
    "\n",
    "def blur_and_canny(img):\n",
    "    blurred = cv.GaussianBlur(img, (7,7), 1)\n",
    "    canny = cv.Canny(blurred, 120, 255, 1)\n",
    "\n",
    "    return canny\n",
    "\n",
    "# return x, y, w, h of the biggest box\n",
    "def find_box(img):\n",
    "    contours = cv.findContours(img, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "    contours = sorted(contours, key=cv.contourArea , reverse=True)[:1]\n",
    "\n",
    "    return cv.boundingRect(contours[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all features\n",
    "DIR = \"./img/avatars\"\n",
    "\n",
    "keypoints, descriptors = [], []\n",
    "for file in os.listdir(DIR):\n",
    "    img = cv.imread(DIR + \"/\" + file, 0)\n",
    "    kp, desc = get_features(img)\n",
    "    keypoints.append(kp)\n",
    "    descriptors.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./img/1.jpeg\n",
      "./img/10.jpeg\n",
      "./img/2.jpg\n",
      "./img/3.jpg\n",
      "./img/4.jpg\n",
      "./img/5.jpg\n",
      "./img/6.png\n",
      "./img/7.jpg\n",
      "./img/8.png\n",
      "./img/9.jpg\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "for i, file in enumerate(os.listdir(\"./img\")):\n",
    "    if file == \"avatars\": \n",
    "        continue\n",
    "\n",
    "    im = \"./img/\" + file\n",
    "    print(im)\n",
    "    img = cv.imread(im, 0)\n",
    "    img = resize_image(img)\n",
    "    train_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all features\n",
    "DIR = \"./img/avatars\"\n",
    "\n",
    "operators = {}\n",
    "for file in os.listdir(DIR):\n",
    "    operators.setdefault(file[:-4], {})\n",
    "\n",
    "    img = cv.imread(DIR + \"/\" + file, 0)\n",
    "    kp, desc = get_features(img)\n",
    "\n",
    "    operators[file[:-4]][\"name\"] = file[:-4]\n",
    "    operators[file[:-4]][\"keypoints\"] = [i.pt for i in kp]\n",
    "    operators[file[:-4]][\"descriptors\"] = desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window 0 = char_378_asbest (38.1)\n",
      "window 1 = char_120_hibisc (59.5)\n",
      "window 2 = char_282_catap (46.1)\n",
      "window 3 = char_120_hibisc (55.5)\n",
      "window 4 = char_123_fang (48.1)\n",
      "window 5 = char_209_ardign (54.9)\n",
      "window 6 = char_423_blemsh (62.7)\n",
      "window 7 = char_452_bstalk (53.5)\n",
      "window 8 = char_149_scave (73.0)\n",
      "window 9 = char_479_sleach (41.0)\n"
     ]
    }
   ],
   "source": [
    "train_img = train_images[9]\n",
    "anchor = find_anchor(train_img)\n",
    "generators = sliding_window(train_img, anchor)\n",
    "\n",
    "for i, window in enumerate(generators):\n",
    "    kp, desc = get_features(window)\n",
    "\n",
    "    prediction = {\"name\": \"\", \"min\": 0}\n",
    "    for name, operator in operators.items():\n",
    "        matches = brute_force_matcher(operator[\"descriptors\"], desc, sort=True)\n",
    "\n",
    "        temp = [i.distance for i in matches[:10]]\n",
    "\n",
    "        if prediction[\"min\"] == 0:\n",
    "            prediction[\"min\"] = mean(temp)\n",
    "        elif prediction[\"min\"] > mean(temp):\n",
    "            prediction[\"min\"] = mean(temp)\n",
    "            prediction[\"name\"] = name\n",
    "    \n",
    "    print(f\"window {i} = {prediction['name']} ({prediction['min']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "- Object detection is expensive\n",
    "- Uploaded images will have various sizes\n",
    "\n",
    "## Solution\n",
    "- Sliding window with anchor based on region of interest\n",
    "\n",
    "## Consideration\n",
    "- Currently the performance is ~500ms per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
